# Social Media Analytics Platform
### End-to-End Data Engineering Pipeline Â· Medallion Architecture Â· Batch + Streaming

[![CI Pipeline](https://github.com/YOUR_USERNAME/social-media-analytics-platform/actions/workflows/ci.yml/badge.svg)](https://github.com/YOUR_USERNAME/social-media-analytics-platform/actions/workflows/ci.yml)
[![dbt Validate](https://github.com/YOUR_USERNAME/social-media-analytics-platform/actions/workflows/dbt_validate.yml/badge.svg)](https://github.com/YOUR_USERNAME/social-media-analytics-platform/actions/workflows/dbt_validate.yml)
[![Docker Build](https://github.com/YOUR_USERNAME/social-media-analytics-platform/actions/workflows/docker_validate.yml/badge.svg)](https://github.com/YOUR_USERNAME/social-media-analytics-platform/actions/workflows/docker_validate.yml)
[![Python 3.11](https://img.shields.io/badge/Python-3.11-blue?logo=python)](https://www.python.org/)
[![Apache Spark](https://img.shields.io/badge/Apache_Spark-3.5-orange?logo=apache-spark)](https://spark.apache.org/)
[![Apache Kafka](https://img.shields.io/badge/Apache_Kafka-3.x-black?logo=apache-kafka)](https://kafka.apache.org/)
[![dbt](https://img.shields.io/badge/dbt_Core-1.7-red?logo=dbt)](https://www.getdbt.com/)
[![Apache Airflow](https://img.shields.io/badge/Apache_Airflow-2.8-017CEE?logo=apache-airflow)](https://airflow.apache.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## Overview

A production-grade **Social Media Analytics Data Platform** built from scratch to demonstrate real-world Data Engineering skills. The platform simulates, ingests, transforms, and serves analytics-ready data from multiple social media sources â€” processing **~5 million events per day** through a complete **Medallion Architecture (Bronze â†’ Silver â†’ Gold)** pipeline.

> **Portfolio Project** â€” Designed to mirror the architecture and practices used in enterprise Data Engineering teams working with tools like Databricks, Snowflake, and Microsoft Fabric.

---

## Architecture

![Architecture Diagram](docs/architecture_diagram.png)

### Data Flow

```
Data Sources (Simulated)
  â”œâ”€â”€ Twitter/X Simulator â”€â”€â–º Kafka Topic â”€â”€â–º Spark Streaming â”€â”€â–º Bronze
  â”œâ”€â”€ YouTube Simulator   â”€â”€â–º Landing Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Bronze
  â””â”€â”€ Reddit Simulator    â”€â”€â–º Landing Zone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Bronze
                                                                      â”‚
                                                              PySpark Job
                                                                      â”‚
                                                                   Silver
                                                                      â”‚
                                                               dbt Models
                                                                      â”‚
                                                                    Gold â”€â”€â–º DuckDB Analytics
```

### Medallion Layers

| Layer | Purpose | Format | Latency |
|-------|---------|--------|---------|
| ðŸ¥‰ **Bronze** | Raw, immutable, as-ingested data | JSON / CSV | < 5 min |
| ðŸ¥ˆ **Silver** | Deduplicated, validated, standardized | Parquet (partitioned) | < 15 min |
| ðŸ¥‡ **Gold** | Business-ready Star Schema | Parquet + DuckDB | < 30 min |

---

## Tech Stack

| Component | Technology | Version |
|-----------|------------|---------|
| Streaming Ingestion | Apache Kafka | 3.x |
| Batch Processing | PySpark | 3.5.x |
| SQL Transformations | dbt Core | 1.7.x |
| Orchestration | Apache Airflow | 2.8.x |
| Object Storage | MinIO (S3-compatible) | Latest |
| Analytical DB | DuckDB | Latest |
| Data Quality | Great Expectations | 0.18.x |
| Containerization | Docker + Compose | Latest |
| CI/CD | GitHub Actions | â€” |
| Language | Python | 3.11 |

---

## Project Structure

```
social-media-analytics-platform/
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ ci.yml                    # Lint + unit tests on every push
â”‚   â”‚   â”œâ”€â”€ dbt_validate.yml          # dbt compile + schema tests
â”‚   â”‚   â””â”€â”€ docker_validate.yml       # Docker build + smoke test
â”‚   â””â”€â”€ ISSUE_TEMPLATE/
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml            # Full local infrastructure
â”‚   â”œâ”€â”€ airflow/Dockerfile
â”‚   â””â”€â”€ spark/Dockerfile
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ simulators/                   # Python Faker data generators
â”‚   â”‚   â”œâ”€â”€ twitter_simulator.py
â”‚   â”‚   â”œâ”€â”€ youtube_simulator.py
â”‚   â”‚   â””â”€â”€ reddit_simulator.py
â”‚   â”œâ”€â”€ ingestion/                    # Batch + streaming ingestion
â”‚   â”‚   â”œâ”€â”€ kafka_producer.py
â”‚   â”‚   â”œâ”€â”€ batch_ingestion.py
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â”œâ”€â”€ spark_jobs/               # PySpark Bronze â†’ Silver jobs
â”‚   â”‚   â”‚   â”œâ”€â”€ bronze_to_silver.py
â”‚   â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚   â””â”€â”€ dbt/                      # dbt project (Silver â†’ Gold)
â”‚   â”‚       â”œâ”€â”€ models/
â”‚   â”‚       â”‚   â”œâ”€â”€ staging/          # stg_twitter__tweets.sql, ...
â”‚   â”‚       â”‚   â”œâ”€â”€ intermediate/     # int_posts_enriched.sql, ...
â”‚   â”‚       â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”‚       â”œâ”€â”€ facts/        # fact_engagements.sql, ...
â”‚   â”‚       â”‚       â””â”€â”€ dimensions/   # dim_users.sql, dim_date.sql, ...
â”‚   â”‚       â”œâ”€â”€ tests/
â”‚   â”‚       â””â”€â”€ dbt_project.yml
â”‚   â””â”€â”€ orchestration/
â”‚       â””â”€â”€ dags/                     # Airflow DAG definitions
â”‚           â”œâ”€â”€ ingestion_pipeline.py
â”‚           â”œâ”€â”€ transformation_pipeline.py
â”‚           â””â”€â”€ daily_refresh.py
â”œâ”€â”€ quality/
â”‚   â””â”€â”€ expectations/                 # Great Expectations checkpoints
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                         # pytest unit tests
â”‚   â””â”€â”€ integration/                  # Integration tests
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ PRD_SocialMedia_Analytics.docx
â”‚   â”œâ”€â”€ TAD_SocialMedia_Analytics.docx
â”‚   â””â”€â”€ data_dictionary.md
â”œâ”€â”€ Makefile                          # Developer convenience commands
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .pre-commit-config.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ requirements-dev.txt
â””â”€â”€ README.md
```

---

## Quick Start

### Prerequisites

- [Docker Desktop](https://www.docker.com/products/docker-desktop/) (v24+)
- [Git](https://git-scm.com/)
- 8GB RAM minimum (16GB recommended)

### Setup in 3 Commands

```bash
# 1. Clone the repository
git clone https://github.com/YOUR_USERNAME/social-media-analytics-platform.git
cd social-media-analytics-platform

# 2. Start all infrastructure services
make up

# 3. Run the full pipeline end-to-end
make pipeline-run
```

### Access the Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow UI | http://localhost:8090 | admin / admin |
| MinIO Console | http://localhost:9001 | minioadmin / minioadmin |
| Spark Master UI | http://localhost:8080 | â€” |

---

## Development Guide

### Available Make Commands

```bash
make help           # Show all available commands
make up             # Start all Docker services
make down           # Stop all Docker services
make logs           # Tail all service logs
make shell          # Open Python shell in Spark container

# Pipeline
make simulate       # Run data simulators (generates ~100K events)
make ingest         # Run batch ingestion jobs
make transform      # Run PySpark Bronze â†’ Silver
make dbt-run        # Run dbt Silver â†’ Gold
make quality        # Run all Great Expectations checkpoints
make pipeline-run   # Run complete pipeline end-to-end

# Development
make lint           # Run flake8 linter
make test           # Run pytest unit tests
make test-cov       # Run tests with coverage report
make dbt-docs       # Generate and serve dbt documentation
make format         # Auto-format code with black + isort
make pre-commit     # Run pre-commit hooks on all files

# Data
make seed           # Load static dimension seed data
make backfill DAYS=7  # Backfill pipeline for last N days
make reset          # Drop all data and restart from scratch (âš ï¸ destructive)
```

### Branching Strategy

```
main              â† protected, requires PR + CI pass
â””â”€â”€ develop       â† integration branch
    â”œâ”€â”€ feature/bronze-ingestion-kafka
    â”œâ”€â”€ feature/silver-pyspark-transform
    â”œâ”€â”€ feature/dbt-star-schema
    â””â”€â”€ fix/dedup-window-function
```

### Commit Convention

```
feat(ingestion): add Kafka producer for tweet simulator
fix(spark): resolve duplicate records in bronze-to-silver job
test(dbt): add schema tests for fact_engagements null checks
docs(readme): update quick start with new make commands
chore(ci): add docker compose smoke test to PR workflow
```

---

## Data Model

### Star Schema (Gold Layer)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  dim_date   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  dim_users   â”œâ”€â”€â”€â”€â”¤  fact_engagements   â”œâ”€â”€â”€â”€â”¤  dim_posts   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  dim_platforms â”‚         â”‚    dim_hashtags     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Business Metrics (Gold Layer)

| Metric | Definition | dbt Model |
|--------|-----------|-----------|
| Engagement Rate | (likes + shares + comments) / impressions Ã— 100 | `fact_post_metrics` |
| Daily Active Users | Unique users with â‰¥1 engagement per day | `mart_daily_active_users` |
| Top Hashtags | Hashtags by total engagement in rolling 7-day window | `mart_trending_hashtags` |
| Brand Share of Voice | Brand mentions / total platform mentions Ã— 100 | `mart_share_of_voice` |

---

## Pipeline Design Principles

- **Idempotency** â€” Every pipeline job is safe to re-run without creating duplicates
- **Schema-on-Write** â€” Strict schema enforcement at Silver layer ingestion
- **Quality Gates** â€” Great Expectations blocks pipeline promotion on data quality failure
- **Observability** â€” All jobs emit structured JSON logs with `run_id`, `records_processed`, `duration_seconds`
- **Portability** â€” MinIO (local) is 100% API-compatible with AWS S3 â€” zero code changes to deploy to cloud
- **Reproducibility** â€” Full environment starts with `docker-compose up` â€” no manual dependencies

---

## CI/CD Pipeline

Every push triggers automated checks:

```
Push / PR
    â”‚
    â”œâ”€â”€ ci.yml
    â”‚   â”œâ”€â”€ flake8 lint
    â”‚   â”œâ”€â”€ pytest unit tests
    â”‚   â””â”€â”€ coverage report (min 80%)
    â”‚
    â”œâ”€â”€ dbt_validate.yml
    â”‚   â”œâ”€â”€ dbt deps
    â”‚   â”œâ”€â”€ dbt compile
    â”‚   â””â”€â”€ dbt test (schema + data tests)
    â”‚
    â””â”€â”€ docker_validate.yml (PRs to main only)
        â”œâ”€â”€ docker compose build
        â”œâ”€â”€ docker compose up --wait
        â””â”€â”€ smoke test (health checks)
```

---

## Documentation

| Document | Description |
|----------|-------------|
| [PRD](docs/PRD_SocialMedia_Analytics.docx) | Problem definition, requirements, project scope |
| [TAD](docs/TAD_SocialMedia_Analytics.docx) | Technical architecture, ADRs, data model, infrastructure |
| [Data Dictionary](docs/data_dictionary.md) | Column-level documentation for all tables |
| [dbt Docs](http://localhost:8080) | Auto-generated dbt lineage and model documentation (run `make dbt-docs`) |

---

## Implementation Status

| Step | Component | Status | Tests |
|------|-----------|--------|-------|
| 1 | Problem Definition (PRD) | âœ… Complete | â€” |
| 2 | Architecture Design (TAD + Star Schema) | âœ… Complete | â€” |
| 3 | Repository Setup + CI/CD skeleton | âœ… Complete | â€” |
| 4 | Docker Infrastructure (7 services) | âœ… Complete | â€” |
| 5 | Data Simulators (Twitter, YouTube, Reddit) | âœ… Complete | 25 tests |
| 6 | Ingestion Layer (Kafka consumer + Batch) | âœ… Complete | 25 tests |
| 7 | Transformation (PySpark Bronzeâ†’Silver + dbt) | âœ… Complete | 30 tests |
| 8 | Orchestration (3 Airflow DAGs) | âœ… Complete | 41 tests |
| 9 | Data Quality (7 GE expectation suites) | âœ… Complete | 44 tests |
| 10 | Integration + Final Packaging | âœ… Complete | 52 tests |

**Total: 217 tests across all layers Â· 87 project files Â· 51 source modules**

## Roadmap

- [x] Phase 1: Problem Definition (PRD)
- [x] Phase 2: Architecture Design (TAD + Diagram)
- [x] Phase 3: Repository Setup
- [x] Phase 4: Infrastructure (Docker Compose)
- [x] Phase 5: Data Simulators
- [x] Phase 6: Ingestion Layer (Kafka + Batch)
- [x] Phase 7: Transformation Layer (PySpark + dbt)
- [x] Phase 8: Orchestration (Airflow DAGs)
- [x] Phase 9: Data Quality (Great Expectations)
- [ ] Phase 10: CI/CD (GitHub Actions â€” full pipeline)
- [ ] Phase 11: Observability (dashboards)
- [ ] Phase 12: Portfolio Polish

---

## License

This project is licensed under the MIT License â€” see the [LICENSE](LICENSE) file for details.

---

<p align="center">Built with focus on real-world Data Engineering practices</p>
